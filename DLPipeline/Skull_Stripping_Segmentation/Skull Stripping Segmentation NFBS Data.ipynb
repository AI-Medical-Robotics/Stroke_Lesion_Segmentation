{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ce76b5",
   "metadata": {},
   "source": [
    "# Skull Stripping Segmentation NFBS Data\n",
    "\n",
    "My plan is to use the Neurofeedback Skull-stripped (NFBS) dataset to train a 3D UNet model on that. Then use that trained 3D UNet model to do automated skull stripping on the ATLAS 2.0 dataset later in our lesion segmentation notebook and source code. Skull stripping is a preprocessing step before lesion segmentation.\n",
    "\n",
    "Reference: https://www.analyticsvidhya.com/blog/2021/06/introduction-to-skull-stripping-image-segmentation-on-3d-mri-images/\n",
    "\n",
    "MRI Brain Dataset with skull stripped labels: http://preprocessed-connectomes-project.org/NFB_skullstripped/\n",
    "\n",
    "## NFBS Dataset\n",
    "\n",
    "- 125 participants 21 to 45 years old with a variety of clinical and sub-clinical psychiatric symptoms.\n",
    "- Structural T1-weighted anonymized (de-faced) images) with a single channel.\n",
    "- Brain mask is the ground truth obtained using Beast (Brain extraction based on nonlocal segmentation) method and applying manual edits by domain experts to remove non-brain tissue.\n",
    "- Skull-stripped image is part of the brain stripped from T1 weighted image. It is similar to overlaying masks to actual images.\n",
    "- The resolution is 1 mm3 and each file is NiFTI (.nii.gz) format.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- Prepare NFBS Data\n",
    "- 3D UNet Model Architecture\n",
    "- Train 3D UNet Model\n",
    "- Deploy 3D UNet for Skull Stripping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623ee35",
   "metadata": {},
   "source": [
    "## Prepare NFBS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e658854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1f6c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of voxel= (256, 256, 192)\n"
     ]
    }
   ],
   "source": [
    "nfbs_path = \"/media/james/My Passport/NFBS_Dataset\"\n",
    "# voxel is a 3D MRI object can be split into MRI 2D slice images\n",
    "voxel = nib.load(\"{}/A00028185/sub-A00028185_ses-NFB3_T1w.nii.gz\".format(nfbs_path))\n",
    "\n",
    "print(\"Shape of voxel=\", voxel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e8947",
   "metadata": {},
   "source": [
    "We can see our 3D MRI voxel also can be thought of as a 3D object containing 192 MRI 2D slice images where each image is 256*256. You can think of these images stacked on top of each other.\n",
    "\n",
    "We will create a data frame that contains the location of images and their corresponding masks and skull-stripped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the address of 3 types of files\n",
    "brain_mask = list()\n",
    "brain = list()\n",
    "raw = list()\n",
    "\n",
    "for subdir, dirs, files in os.walk(nfbs_path):\n",
    "    for file in files:\n",
    "        # print(os.path.join(subdir, file))\n",
    "        filepath = subdir + os.sep + file\n",
    "        \n",
    "        if filepath.endswith(\".gz\"):\n",
    "            if \"_brainmask.\" in filepath:\n",
    "                brain_mask.append(filepath)\n",
    "            elif \"_brain.\" in filepath:\n",
    "                brain.append(filepath)\n",
    "            else:\n",
    "                raw.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfabed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfbs_df = pd.DataFrame(\n",
    "    {\"brain_mask\": brain_mask,\n",
    "     \"brain\": brain,\n",
    "     \"raw\": raw\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89f6f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain_mask</th>\n",
       "      <th>brain</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0004372...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0004372...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0004372...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0005609...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0005609...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0005609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0006025...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0006025...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0006025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002818...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002818...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002835...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002835...</td>\n",
       "      <td>/media/james/My Passport/NFBS_Dataset/A0002835...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          brain_mask  \\\n",
       "0  /media/james/My Passport/NFBS_Dataset/A0004372...   \n",
       "1  /media/james/My Passport/NFBS_Dataset/A0005609...   \n",
       "2  /media/james/My Passport/NFBS_Dataset/A0006025...   \n",
       "3  /media/james/My Passport/NFBS_Dataset/A0002818...   \n",
       "4  /media/james/My Passport/NFBS_Dataset/A0002835...   \n",
       "\n",
       "                                               brain  \\\n",
       "0  /media/james/My Passport/NFBS_Dataset/A0004372...   \n",
       "1  /media/james/My Passport/NFBS_Dataset/A0005609...   \n",
       "2  /media/james/My Passport/NFBS_Dataset/A0006025...   \n",
       "3  /media/james/My Passport/NFBS_Dataset/A0002818...   \n",
       "4  /media/james/My Passport/NFBS_Dataset/A0002835...   \n",
       "\n",
       "                                                 raw  \n",
       "0  /media/james/My Passport/NFBS_Dataset/A0004372...  \n",
       "1  /media/james/My Passport/NFBS_Dataset/A0005609...  \n",
       "2  /media/james/My Passport/NFBS_Dataset/A0006025...  \n",
       "3  /media/james/My Passport/NFBS_Dataset/A0002818...  \n",
       "4  /media/james/My Passport/NFBS_Dataset/A0002835...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfbs_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c140d",
   "metadata": {},
   "source": [
    "The next preparation steps we'll perform are:\n",
    "\n",
    "- Bias field correction: bias field signal is a low-frequency and smooth signal that corrupts MRI images especially those produced by old MRI machines. Thus, bias field correction is done on MRIs prior to pushing them into image processing algorithms: segmentation, texture analysis, classification.\n",
    "- Cropping and resizing: due to computational limits of fitting image to model, we reduce MRI image size from (`256*256*192`) to (`96*128*160`). The target size is chosen in a way where most of the skull is captured after cropping and resizing it has a centering effect on images.\n",
    "- Intensity normalization: shifts and scales an image so the pixels have a zero mean and unit variance. This helps the model converge faster by removing scale in-variance.\n",
    "- Data Generator to Feed Data to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6794ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing():\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        self.raw_index = list()\n",
    "        self.mask_index = list()\n",
    "    def bias_correction(self):\n",
    "        bias_dir = \"bias_correction\"\n",
    "        bias_path = os.path.join(nfbs_path, bias_dir)\n",
    "        if not os.path.exists(bias_path):\n",
    "            os.makedirs(bias_path)\n",
    "        n4 = N4BiasFieldCorrection()\n",
    "        n4.inputs.dimensions = 3\n",
    "        n4.inputs.shrink_factor = 3\n",
    "        n4.inputs.n_iterations = [20, 10, 10, 5]\n",
    "        index_corr = list()\n",
    "        for i in tqdm(range(len(self.data))):\n",
    "            n4.inputs.input_image = self.data.raw.iloc[i]\n",
    "            n4.inputs.output_image = bias_dir + os.sep + str(i) + \".nii.gz\"\n",
    "            index_corr.append(bias_dir + os.sep + str(i) + \".nii.gz\")\n",
    "            res = n4.run()\n",
    "        index_corr = [bias_dir + os.sep + str(i) + \".nii.gz\" for i in range(125)]\n",
    "        data[\"bias_corr\"] = index_corr\n",
    "        print(\"bias corrected voxels stored at : {}/\".format(bias_dir))\n",
    "    def resize_crop(self):\n",
    "        # reducing the size of image due to memory constraints\n",
    "        self.rcrop_dir = \"resized\"\n",
    "        rcrop_path = os.path.join(nfbs_path, rcrop_dir)\n",
    "        #reducing size of image from 256*256*192 to 96*128*160\n",
    "        target_shape = np.array((96,128,160))\n",
    "        new_resolution = [2,]*3\n",
    "        new_affine = np.zeros((4,4))\n",
    "        new_affine[:3,:3] = np.diag(new_resolution)\n",
    "        # putting point 0,0,0 in the middle of the new volume - this\n",
    "        # could be refined in the future\n",
    "        new_affine[:3,3] = target_shape*new_resolution/2.*-1\n",
    "        new_affine[3,3] = 1.\n",
    "        raw_index = list()\n",
    "        mask_index = list()\n",
    "        # resizing both image and mask and storing in folder\n",
    "        for i in range(len(data)):\n",
    "            downsampled_and_cropped_nii = resample_img(\n",
    "                self.data.data_corr.iloc[i], target_affine=new_affine,\n",
    "                target_shape=target_shape, interpolation=\"nearest\")\n",
    "            downsampled_and_cropped_nii.to_filename(\n",
    "                rcrop_dir + os.sep + \"raw\" + str(i) + \".nii.gz\")\n",
    "            self.raw_index.append(rcrop_dir + os.sep + \"raw\" + str(i) + \".nii.gz\")\n",
    "            \n",
    "            downsampled_and_cropped_nii = resample_img(\n",
    "                self.data.brain_mask.iloc[i], target_affine=new_affine,\n",
    "                target_shape=target_shape, interpolation=\"nearest\")\n",
    "            downsampled_and_cropped_nii.to_filename(\n",
    "                rcrop_dir + os.sep + \"mask\" + str(i) + \".nii.gz\")\n",
    "            self.mask_index.append(rcrop_dir + os.sep + \"mask\" + str(i) + \".nii.gz\")\n",
    "        return self.raw_index, self.mask_index\n",
    "    \n",
    "    def intensity_normalization(self):\n",
    "        for i in self.raw_index:\n",
    "            image = sitk.ReadImage(i)\n",
    "            resacleFilter = sitk.RescaleIntensityImageFilter()\n",
    "            resacleFilter.SetOutputMaximum(255)\n",
    "            resacleFilter.SetOutputMinimum(0)\n",
    "            image = resacleFilter.Execute(image)\n",
    "            sitk.WriteImage(image, i)\n",
    "        print(\"Normalization done. Voxels stored at: {}/\".format(self.rcrop_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd65cd",
   "metadata": {},
   "source": [
    "## 3D UNet Model Architecture\n",
    "\n",
    "We've built our preprocessing class, we can begin modeling. First we do a train and test split. Then we use a custom data generator to feed the input MRIs into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
